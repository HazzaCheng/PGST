                                  More Booze You Can Use            When we last heard from them, the members of the                  Slate                beer-testing team were coping with lagers and trying to seeif they could taste the 3-to-1 price difference between the most- andleast-expensive brands. (Click for a wrap-up of the first round of beertasting.) The answer was: They found one beer they really liked, SamuelAdams Boston Lager , and one they really hated, imported Grolsch fromHolland. Both were expensive beers--Grolsch was the most expensive in thetest--and otherwise the testers had a hard time telling beers apart. Themembers of the team, as noted in the original article, all hold day jobs atMicrosoft, mainly as designers, managers, and coders for Microsoft Word.            The point of the second test was not to find thedifference between cheap and expensive beers but instead to compare a varietyof top-of-the-line beers. Was there one kind the tasters preferredconsistently? Could they detect any of the subtleties of brewing style andprovenance that microbrew customers pay such attention to when choosing someDoppelbock over a cream ale?            Since the tasting panel had left the first roundgrumbling that cheap lagers were not a fair test of their abilities, thissecond round of testing was advertised to the panel as a reward. Every beer inRound 2 would be a fancy beer. A microbrew. A "craft beer." A prestigiousimport. These were the kinds of beer the panel members said they liked--and theones they said they were most familiar with. One aspect of the reward was thatthey would presumably enjoy the actual testing more--fewer rueful beerdescriptions along the lines of "urine" or "get it away!" were expected than inthe first round. The other aspect of anticipated reward was the panelists'unspoken but obvious assumption that this time they would "do better" on thetest. Intellectual vanity being what it is, people who had fought for and wonjobs at Microsoft and who still must fight every six months for primacy on theemployee-ranking scale (which determines--gasp!--how many new stock optionsthey receive) would assume that their skill as tasters was on trial, just asmuch as the beer was. Of course they were right, which is what made this roundas amusing to administer as the first one had been.            Here is what happenedand what it meant:                           1.                Procedure. This wassimilar in most ways to the experimental approach of Round 1. The nine testerswho showed up were a subset of the original 12. The missing three dropped outwith excuses of "my wife is sick" (one person) and "meeting is running long"(two).            As before, each tester found before him on a table10 red plastic cups, labeled A through J. Each cup held 3 ounces of one of thebeers. The A-to-J labeling scheme was the same for all testers. Instead ofsaltines for palate-cleansing, this time we had popcorn and nuts. As theybegan, the tasters were given these and only these clues:                           that the flight included one "holdover" beer from the previous round(Sam Adams);               that it included at least one import (Bass);               that it included at least one macrobrew ,specifically, a member of the vast Anheuser-Busch family (MichelobHefeweizen).                        After sampling all beers, the tasters rated them asfollows:                                             Overall quality points, from zero to 100, reflecting theirpersonal, subjective fondness for the beer.                                 Descriptions of and comments about eachbeer's taste--"smooth and nutty," "too strong," etc. If the first ranking was ameasure of how good each beer was, this was an attempt to explain what made itgood.                                 Best                   and Worst , one of each from the group.                                 Name                   that beer! The tasters were told that some ofthe drinks were Hefeweizens, some might be IPAs (India pale ales), some mightbe bitters, and so on. They were asked to put each beer in its propercategory--and to name a specific brewery and brand if they could. The idea herewas to test the veteran beer drinkers' claim to recognize the distinctivetastes of famous brands. (To see all the grids for all the beers, click .)                                       2.                Philosophy. The firstround of testing was All Lager. This second round was All Fancy, and Mainly NotLager. As several correspondents (for instance, the of Best AmericanBeers ) have helpfully pointed out, the definition of lager provided lasttime was not exactly "accurate." If you want to stay within the realm oftextbook definitions, a lager is a beer brewed a particular way--slowly, atcool temperatures, with yeast that settles on the bottom of the vat. This is incontrast with an ale, which is brewed faster, warmer, and with the yeast ontop. By this same reasoning, lagers don't have to be light-colored,weak-flavored, and watery, as mainstream American lagers are. In principle,lagers can be dark, fierce, manly. Therefore, the correspondents suggest, itwas wrong to impugn Sam Adams or Pete's Wicked for deceptivelabeling, in presenting their tawnier, more flavorful beers as lagers too.            To this the beerscientist must say: Book-learning is fine in its place. But let's be realistic.Actual drinking experience teaches the American beer consumer that a) all cheapbeers are lagers; and b) most lagers are light-colored and weak. The first testwas designed to evaluate low-end beers and therefore had to be lager-centric.This one is designed to test fancy beers--but in the spirit of open-mindednessand technical accuracy, it includes a few "strong" lagers too.                           3.               Materials. The 10 test beers were chosen with several goals in mind:                           To cover at least a modest range of fancy beer types--extra special bitter,India pale ale, Hefeweizen, and so on.               To include both imported and domestic beers. Among the domestic microbrews,there's an obvious skew toward beers from the Pacific Northwest. But asMicrosoft would put it, that's a feature not a bug. These beers all came fromthe Safeway nearest the Redmond, Wash., "main campus" of Microsoft, andmicrobrews are supposed to be local.               To include one holdover from the previous test, as a scientific control onour tasters' preferences. This was Sam Adams , runaway winner of Round1.               To include one fancy product from a monster-scale U.S. mass brewery, to seeif the tasters liked it better or worse than the cute little microbrews. Thiswas Michelob Hefeweizen , from the pride of St. Louis,Anheuser-Busch.                        Click for pricing information and pre-quaffingevaluations. The beers tasted were:                                                                                                                                                             4. DataAnalysis.                                       a)                Best and Worst. Comparedto the lager test, we would expect the range of "best" choices to be morevaried, since all the tested beers were supposed to be good. This expectationwas most dramatically borne out in the "Best and Worst" rankings.            The nine tasters cast a total of nine Worst votesand 11.5 Best votes. (Tester No. 1 turned in a sheet with three Bestselections, or two more than his theoretical quota. Tester No. 4 listed a Bestand a Best-minus, which counted as half a vote.)             The results were clearest at the bottom: threeWorsts for Pyramid Hefeweizen , even though most comments about the beerwere more or less respectful. ("Bitter, drinkable.") But at the top and middlethe situation was muddier:                         There were three Bestsfor Full Sail ESB , which most of the tasters later said they weren'tfamiliar with, and 2.5 for Redhook IPA , which all the tasters knew. Buteach of these also got a Worst vote, and most of the other beers had a mixedreading. So far, the tasters are meeting expectations, finding something tolike in nearly all these fancy beers.                           b)                Overall preferencepoints. Here the complications increase. The loser was again apparent:Pyramid Hefeweizen came in last on rating points, as it had in theBest/Worst derby. But the amazing dark horse winner was MichelobHefeweizen . The three elements of surprise here, in ascending order ofunexpectedness, are:                           This best-liked beer belonged to the same category, Hefeweizen, as theleast-liked product, from Pyramid.               This was also the only outright Anheuser-Busch product in thecontest (the Redhooks are 75 percent A-B free). It is safe to say that alltasters would have said beforehand that they would rank an American macrobrewlast, and Anheuser-Busch last of all.               Although it clearly won on overall preference points, Michelob Hefeweizenwas the only beer not to have received a single "Best" vote.                        The first two anomalies can be written off astestament to the power of a blind taste test. The third suggests an importantdifference in concepts of "bestness." Sometimes a product seems to be the bestof a group simply because it's the most unusual or distinctive. This is whyvery high Wine Spectator ratings often go to wines that mainly tasteodd. But another kind of bestness involves an unobtrusive, day-in day-outacceptability. That seems to be Michelob Hefe 's achievement here: noone's first choice, but high on everyone's list. Let's go to the charts:            This table shows how the beers performed on "rawscore"--that is, without the advanced statistical adjustment of throwing outthe highest and lowest score each beer received.                         Next, we have "corrected average preference points,"throwing out the high and low marks for each beer. The result is basically thesame:                         It is worth noting thefate of Sam Adams on these charts. Here it ends up with a score of lessthan 61. These were the numbers awarded by the very same tasters who gave it acorrected preference rating of 83.33 the last time around--and 10 "Best" votes,vs. one Best (and one Worst) this time. The shift in Bests is understandableand demonstrates the importance of picking your competition. The severe drop inpreference points illustrates more acutely the ancient principle of being a bigfish in a small pond. These same tasters thought that Sam Adams was objectivelymuch better when it was surrounded by Busch and Schmidt's.                           c)                Value rankings. Lasttime this calculation led to what the colorful French would call abouleversement. One of the cheapest beers, Busch, which had been in thelower ranks on overall preference points, came out at the top onvalue-for-money ratings, because it was so cheap. The big surprise now is thatthe highest-rated beer was also the cheapest one, Michelob Hefe ,so the value calculation turned into a rout:                                        Pyramid               Hefeweizen was expensive on top of being unpopular, so its position atthe bottom was hammered home--but not as painfully as that of BassAle . Bass had been in the respectable lower middle class of thepreference rankings, so its disappointing Val-u-meter showing mainly reflectsthe fact that it was the only beer not on "sale" and therefore by far thecostliest entry in the experiment.                           d)                Taster skill. As membersof the tasting panel began to suspect, they themselves were being judged whilethey judged the beer. One of the tasters, No. 7, decided to live dangerouslyand give specific brands and breweries for Samples A through J. This man wasthe only panel member whose job does not involve designing Microsoft Word--andthe only one to identify two or more of the beers accurately and specifically.(He spotted Redhook IPA and Redhook ESB.) The fact that the beers correctlyidentified were the two most popular microbrews in the Seattle area suggeststhat familiarity is the main ingredient in knowing your beer.            Many others were simply lost. Barely half thetasters, five of nine, recognized that Michelob Hefeweizen                was aHefeweizen. Before the test, nine of nine would have said that picking out aHefe was easy, because of its cloudy look and wheaty flavor. Three tastersthought Sam Adams was an IPA ; two thought Redhook's IPA was aHefeweizen. In fairness, six of nine testers identified PyramidHefeweizen as a Hefe, and six recognized Full Sail ESB as a bitter.Much in the fashion of blind men describing an elephant, here is a how thetesters handled Sam Adams Boston Lager :                                        5.Implications                and Directions for Future Research. Science doesnot always answer questions; often, it raises many new ones. This excursioninto beer science mainly raises the question: What kind of people are we?            If we are Gradgrind-like empiricists, living ourlife for "welfare maximization" as described in introductory econ. courses, theconclusion is obvious. We learned from the first experiment to buyeither Sam Adams (when we wanted maximum lager enjoyment per bottle)or Busch (for maximum taste and snob appeal per dollar). From thissecond round we see an even more efficient possibility: Buy MichelobHefeweizen and nothing else, since on the basis of this test it's the bestliked and the cheapest beer. By the way, if there is a single companywhose achievements the testing panel honored, it would beAnheuser-Busch . From its brewing tanks came two of the double-crownwinners of the taste tests: plain old Busch , the Taste-o-meterand Snob-o-meter victor of Round 1, and Michelob Hefeweizen , thepreference-point and Val-u-meter winner this time.            But, of course, there is another possibility: thatwhat is excluded in a blind taste test is in fact what we want, and are happyto pay for, when we sit down with a beer. The complicated label, the fancybottle, the exotic concept that this beer has traveled from some far-off cornerof Bohemia or even the Yakima Valley--all this may be cheap at the$1.25-per-pint cost difference between the cheapest and the most expensivebeers. In elementary school, we all endured a standard science experiment: Ifyou shut your eyes and pinch your nose closed, can you tell any difference inthe taste of a slice of apple, of carrot, of pear? You can't--but that doesn'tmean that from then on you should close your eyes, hold your nose, and chew acheap carrot when you feel like having some fruit. There is a time and placefor carrots, but also for juicy pears. There is a time for Busch, but also forFull Sail "Equinox."            For scientists who want to continue this work athome, here are a few suggestions for further research:                           Tell the testers ahead of time what beers they will be drinking. Ask themto rank the beers, 1 through 10, based on how well they like them. Then comparethe list with the "revealed preferences" that come from the blind test.               As a variation, show them the list ahead of time and ask them to pick outthe beer they know they love and the one they know they hate. Then compare thiswith the "after" list.               If you're going to test imported lagers, try Foster's or Corona rather thanGrolsch.               Remember to stay strictly in the scientist's role. Don't take the testyourself.                              